{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nimport pandas as pd\nimport numpy as np\nfrom bs4 import BeautifulSoup","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to make a soup object out of a URL.\n\ndef soup_object_maker(URL):\n    request = requests.get(URL).text\n    soup_object = BeautifulSoup(request, 'lxml')\n    return soup_object","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to get the name of the table you are accessing. \n\ndef get_column_names(soup_object):\n    DF_Name = soup_object.find('div', class_ ='stats-header__lower__title').text\n    DF_Columns_List = soup_object.thead.find_all('th')\n    return DF_Name, DF_Columns_List","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to extract the text from the soup object and add it to the dataframe.\n\ndef get_rows(soup_object):\n    Rows = []\n    DF_Rows = soup_object.find_all('tr')\n    for columns in DF_Rows[1:]:\n        clear = []\n        for rows in columns.find_all('td'):\n            clear.append(str(rows.text))\n        Rows.append(clear)\n    return Rows","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to save the dataframe and column names to a csv.\n\ndef save_to_csv(DataFrame, DF_Name):\n    File_Name = '/Users/jonathanlewis/Desktop/NCAA/{}'.format(str(DF_Name))\n    DataFrame.to_csv(File_Name, mode='a', header=True)\n    \n#Function to save the dataframe to a csv. (rows ONLY!)\n\ndef save_to_csv_body(DataFrame, DF_Name):\n    File_Name = '/Users/jonathanlewis/Desktop/NCAA/{}'.format(str(DF_Name))\n    DataFrame.to_csv(File_Name, mode= 'a', header=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions to tie together all other functions.\ndef get_data(url_list):\n    soup_object = soup_object_maker(url_list)\n    DF_Name, DF_Columns_List = get_column_names(soup_object)\n    DF_Rows = get_rows(soup_object)\n    DF_Columns = (str(x.text)for x in DF_Columns_List)\n    Data_Frame = pd.DataFrame(DF_Rows, columns=[*DF_Columns])\n    save_to_csv(Data_Frame, DF_Name)\n\n#Function the same as above, just calls the \"save_to_csv_body\" function instead of the \"save_to_csv\" function.\ndef get_data_body(url_list):\n    soup_object = soup_object_maker(url_list)\n    DF_Name, DF_Columns_List = get_column_names(soup_object)\n    DF_Rows = get_rows(soup_object)\n    DF_Columns = (str(x.text)for x in DF_Columns_List)\n    Data_Frame = pd.DataFrame(DF_Rows, columns=[*DF_Columns])\n    save_to_csv_body(Data_Frame, DF_Name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting the initial list of urls to scrape.\n\nWeb_Site = 'https://www.ncaa.com/stats/football/fbs'\n\nStats_Dropdown_Soup_Object = soup_object_maker(Web_Site)\nStats_Dropdown_List = Stats_Dropdown_Soup_Object.find('select', class_='stats-dropdown-filter stats-select form-select')\n\nlink_list = []\n\nfor item in Stats_Dropdown_List:\n    https = 'https://www.ncaa.com'\n    link_list.append(https+item['value'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to determine the number of pages a url has.\n\ndef page_counter(URL_List):\n    soup_object = soup_object_maker(URL_List)\n    LI = soup_object.find_all('li', class_='stats-pager__li')\n    return(len(LI))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Because each url had \"pages\", this bit of code figures out how many pages there are and adds a url for each.\n\nPagenated_URL_List = []\n\nfor item in New_List:\n    LI_Count = page_counter(item)\n    LI_range = range(2, LI_Count)\n    Pagenated_URL_List.append(item)\n    if LI_Count > 0:\n        for page in LI_range:\n            Pagenated_URL_List.append(item+'/p{}'.format(page))\n    else:\n        pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calling the function on the list of urls. \n\nfor item in Pagenated_URL_List:\n    if item in link_list:\n        get_data(item)\n    else:\n        get_data_body(item)","metadata":{},"execution_count":null,"outputs":[]}]}